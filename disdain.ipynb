{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DIScriminator DisAgreement INtrinsic Reward (DISDAIN), a self-contained JAX implementation\n",
        "\n",
        "This is a simplified version of the code used in [Learning more skills through optimistic exploration][ICLR2022] (appearing at ICLR 2022).\n",
        "\n",
        "This Colab trains an agent with a tabular Q function and a tabular discriminator ensemble on a scaled down version of the Four Rooms environment. It will parallelize across all available devices. We recommend training on a GPU backend or Colab Pro TPU backend.\n",
        "\n",
        "The environment has 24 states. With 8 transitions, all but one state is reachable from the initial state in the top left corner. This means that at most 23 distinguishable skills can be learned.\n",
        "\n",
        "With the default hyperparameters on a single accelerator, skill learning with DISDAIN achieves approximately 15 effective skills in 500,000 steps and 21 effective skills in 1,000,000 steps, while a matched hyperparameter baseline (with discriminator ensemble disabled) attains approximately 11 effective skills through the course of training (approximately 12 if deriving rewards from an ensemble average, without the DISDAIN bonus). Each agent trains in approximately 12 minutes on the default GPU backend.\n",
        "\n",
        "This implementation broadly matches the setting of the Four Rooms experiments from the paper, with the following differences:\n",
        "\n",
        "* the Four Rooms grid world has been scaled down;\n",
        "* trajectories are generated online, rather than placed in and sampled from a replay buffer;\n",
        "* the learning rate and bonus weight have been re-tuned in light of the above.\n",
        "\n",
        "\n",
        "\n",
        "[ICLR2022]: https://openreview.net/forum?id=cU8rknuhxc\n",
        "\n",
        "## LICENSE\n",
        "\n",
        "Copyright 2022 DeepMind Technologies Limited.\n",
        "\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ],
      "metadata": {
        "id": "DzcYWBsGQRwZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "l0y3jWd3sz1e"
      },
      "outputs": [],
      "source": [
        "# @title (Optional) Install JAX 0.3.10 with CUDA support\n",
        "!pip install --upgrade pip\n",
        "# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n",
        "!pip install --upgrade \"jax[cuda]==0.3.10\" -f https://storage.googleapis.com/jax-releases/jax_releases.html  # Note: wheels only available on linux."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install a more recent matplotlib (may require a runtime restart)\n",
        "# Fixes the use of tight_layout() with suptitle(...) below\n",
        "# See https://matplotlib.org/stable/users/prev_whats_new/whats_new_3.3.0.html#tight-layout-now-supports-suptitle\n",
        "!pip install 'matplotlib>=3.3.0'\n",
        "\n",
        "import matplotlib\n",
        "if tuple(map(int, matplotlib.__version__.split('.')[:2])) < (3, 3):\n",
        "  raise RuntimeError('Outdated matplotlib detected. '\n",
        "                     'Restart the runtime before proceeding.')"
      ],
      "metadata": {
        "id": "Fn3aOBCpWcsf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FZDJZViVLivE"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import functools\n",
        "import dataclasses\n",
        "import datetime\n",
        "import math\n",
        "import operator\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List, NewType, Optional, Tuple, TypeVar, Union\n",
        "from typing_extensions import Protocol\n",
        "try:\n",
        "  import chex\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/deepmind/chex.git@v0.1.3\n",
        "  import chex\n",
        "try:\n",
        "  import haiku as hk\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/deepmind/dm-haiku.git@v0.0.6\n",
        "  import haiku as hk\n",
        "try:\n",
        "  import optax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/deepmind/optax.git@v0.1.2\n",
        "  import optax\n",
        "try:\n",
        "  import rlax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/deepmind/rlax.git@b652c45382605d3bf2c7db837364deda19819fce\n",
        "  import rlax\n",
        "\n",
        "\n",
        "try:\n",
        "  import jax.tools.colab_tpu\n",
        "  try:\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "  except KeyError:  # Not on a TPU Colab backend.\n",
        "    pass\n",
        "except ImportError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TkRfPuhPLn29"
      },
      "outputs": [],
      "source": [
        "# @title Environment Dynamics\n",
        "\n",
        "# Define some type aliases. These don't actually help us in Colab, but if\n",
        "# we were to run through a type checker we'd get helpful errors if we tried\n",
        "# to pass actions where states were expected, etc.\n",
        "Actions = NewType('Actions', chex.Array)\n",
        "States = NewType('States', chex.Array)\n",
        "LatentCodes = NewType('LatentCodes', chex.Array)\n",
        "LatentLogProbs = NewType('LatentLogProbs', chex.Array)\n",
        "QValues = NewType('QValues', chex.Array)\n",
        "\n",
        "\n",
        "class Policy(Protocol):\n",
        "  \"\"\"Interface for an (unconditional) policy.\"\"\"\n",
        "  def __call__(self, rng_key: chex.PRNGKey, states: States) -> Actions:\n",
        "    \"\"\"Generate an action from the environment state and an RNG state.\n",
        "\n",
        "    Args:\n",
        "      rng_key: PRNGKey to use for any stochasticity in action selection.\n",
        "      states: A batch of environment states.\n",
        "    Returns:\n",
        "      A batch of integer actions, with shape equal to `states.shape[:-1]`.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Unroll:\n",
        "  \"\"\"The result of unrolling environment dynamics according to a policy.\n",
        "\n",
        "  `states` is expected to have one more element along its leading dimension\n",
        "  than `actions`, to account for the state reached after the last action.\n",
        "  \"\"\"\n",
        "  actions: Actions\n",
        "  states: States\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class UniformRandomPolicy:\n",
        "  \"\"\"A trivial random uniform policy for the above environment.\n",
        "\n",
        "  Defined as a dataclass so that the hash depends only on the number of\n",
        "  actions. Defined in the cell above the plotting code so that the class\n",
        "  isn't redefined every time the cell is refreshed, which would invalidate\n",
        "  the JIT compile cache for `env.unroll`.\n",
        "  \"\"\"\n",
        "  num_actions: int\n",
        "\n",
        "  def __call__(self, rng_key: chex.PRNGKey, states: States) -> Actions:\n",
        "    action_shape = states.shape[:-1]\n",
        "    return jax.random.randint(rng_key, action_shape, 0, self.num_actions)\n",
        "\n",
        "\n",
        "def map_positions(\n",
        "    binary_map: np.ndarray,\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Convert a map in the form of a binary mask into a position list.\n",
        "\n",
        "  Args:\n",
        "    binary_map: A rank 2 binary array.\n",
        "\n",
        "  Returns:\n",
        "    A 2-d array with 2 columns representing coordinates of positions.\n",
        "  \"\"\"\n",
        "  chex.assert_rank(binary_map, 2)\n",
        "  chex.assert_type(binary_map, bool)\n",
        "  return np.array([\n",
        "      list(c) for c in np.where(binary_map)\n",
        "  ]).T\n",
        "\n",
        "\n",
        "FOUR_ROOMS_GRID = np.array([\n",
        "    [c == '#' for c in line.strip()]\n",
        "    for line in \"\"\"\n",
        "      #######\n",
        "      #  #  #\n",
        "      #     #\n",
        "      ## #  #\n",
        "      #  # ##\n",
        "      #  #  #\n",
        "      #     #\n",
        "      #######\n",
        "    \"\"\".strip().split('\\n')\n",
        "])\n",
        "\n",
        "\n",
        "FOUR_ROOMS_STATES = map_positions(np.logical_not(FOUR_ROOMS_GRID))\n",
        "NUM_ACTIONS = 5\n",
        "NOOP, UP, DOWN, LEFT, RIGHT = range(NUM_ACTIONS)\n",
        "\n",
        "\n",
        "def transition_tensor(grid: chex.ArrayNumpy) -> chex.ArrayNumpy:\n",
        "  \"\"\"Return a binary tensor specifying the transition dynamics.\n",
        "\n",
        "  The resulting tensor has `T[a, j, i] == 1` if taking action `a`\n",
        "  in state `i` lands you in state `j`, with all other values equal\n",
        "  to zero.\n",
        "\n",
        "  Args:\n",
        "    grid: A 2-dimensional NumPy array of 0s and 1s, 1s indicate the position\n",
        "      of impassable barriers.\n",
        "\n",
        "  Returns:\n",
        "    A 3-dimensional binary tensor, with position a,j,i containing a 1 if\n",
        "    taking action a in state i leads to state j, 0 otherwise.\n",
        "  \"\"\"\n",
        "  states = map_positions(np.logical_not(grid))\n",
        "  row, col = states.T\n",
        "  id_map = -grid.astype(np.int32)\n",
        "  num_states = states.shape[0]\n",
        "  num_rows, num_cols = grid.shape\n",
        "  sequential_states = np.arange(num_states)\n",
        "  id_map[row, col] = sequential_states\n",
        "\n",
        "  # Compass directions plus no-op.\n",
        "  transitions = np.zeros((NUM_ACTIONS, num_states, num_states),\n",
        "                         dtype=np.int8)\n",
        "  transitions[NOOP] = np.eye(num_states, dtype=np.int8)\n",
        "\n",
        "  up_state = np.where(\n",
        "      id_map[np.clip(row - 1, 0, num_rows - 1), col] >= 0,\n",
        "      id_map[np.clip(row - 1, 0, num_rows - 1), col],\n",
        "      sequential_states,\n",
        "  )\n",
        "  transitions[UP, up_state, sequential_states] = 1\n",
        "\n",
        "  down_state = np.where(\n",
        "      id_map[np.clip(row + 1, 0, num_rows - 1), col] >= 0,\n",
        "      id_map[np.clip(row + 1, 0, num_rows - 1), col],\n",
        "      sequential_states,\n",
        "  )\n",
        "  transitions[DOWN, down_state, sequential_states] = 1\n",
        "\n",
        "  left_state = np.where(\n",
        "      id_map[row, np.clip(col - 1, 0, num_cols - 1)] >= 0,\n",
        "      id_map[row, np.clip(col - 1, 0, num_cols - 1)],\n",
        "      sequential_states,\n",
        "  )\n",
        "  transitions[LEFT, left_state, sequential_states] = 1\n",
        "\n",
        "  right_state = np.where(\n",
        "      id_map[row, np.clip(col + 1, 0, num_cols - 1)] >= 0,\n",
        "      id_map[row, np.clip(col + 1, 0, num_cols - 1)],\n",
        "      sequential_states,\n",
        "  )\n",
        "  transitions[RIGHT, right_state, sequential_states] = 1\n",
        "  return transitions\n",
        "\n",
        "\n",
        "@chex.dataclass(mappable_dataclass=False, frozen=True)\n",
        "class GridWorld:\n",
        "  \"\"\"Defines dynamics for a simple grid world.\n",
        "\n",
        "  This class is stateless but for its configuration, and simply provides\n",
        "  methods to generate/mutate states.\n",
        "  \"\"\"\n",
        "  transition_tensor: chex.Array\n",
        "\n",
        "  def __hash__(self) -> int:\n",
        "    return id(self)\n",
        "\n",
        "  @property\n",
        "  def state_dim(self) -> int:\n",
        "    return self.transition_tensor.shape[-1]\n",
        "\n",
        "  @property\n",
        "  def num_actions(self) -> int:\n",
        "    \"\"\"Return the number of actions available in the environment.\"\"\"\n",
        "    return self.transition_tensor.shape[0]\n",
        "\n",
        "  def initialize(\n",
        "      self,\n",
        "      shape: chex.Shape,\n",
        "  ) -> jnp.ndarray:\n",
        "    \"\"\"Generate a batch of states with state index 0.\n",
        "\n",
        "    Args:\n",
        "      shape: An integer or tuple of integers indicating the leading\n",
        "        dimensions (e.g. batch size).\n",
        "    Returns:\n",
        "      An array with `size` as its leading dimension(s) and the number of\n",
        "      states as its final dimension.\n",
        "    \"\"\"\n",
        "    return jax.nn.one_hot(\n",
        "        jnp.zeros(shape, dtype=jnp.int8),\n",
        "        self.transition_tensor.shape[-1],\n",
        "        dtype=jnp.int8,\n",
        "    )\n",
        "\n",
        "  @functools.partial(jax.named_call, name='GridWorld.transition')\n",
        "  def transition(\n",
        "      self,\n",
        "      state: States,\n",
        "      action: Actions,\n",
        "  ) -> States:\n",
        "    \"\"\"Compute the effect of taking a set of actions in a given set of states.\"\"\"\n",
        "    one_hot_actions = jax.nn.one_hot(action, self.num_actions, dtype=jnp.int8)\n",
        "    return jax.lax.stop_gradient(\n",
        "        jnp.einsum(\n",
        "            '...a,ads,...s->...d',\n",
        "            one_hot_actions,\n",
        "            jnp.array(self.transition_tensor),\n",
        "            state,\n",
        "        )\n",
        "    )\n",
        "\n",
        "  @functools.partial(jax.named_call, name='GridWorld.unroll')\n",
        "  def unroll(\n",
        "      self,\n",
        "      policy: Policy,\n",
        "      rng_keys: chex.PRNGKey,\n",
        "      initial: States,\n",
        "  ) -> Unroll:\n",
        "    \"\"\"Unroll a trajectory from an initial state according to a policy.\n",
        "\n",
        "    Args:\n",
        "      policy: A callable taking a PRNGKey and a batch of states of the\n",
        "        environment and returning actions.\n",
        "      rng_keys: A pre-split PRNGKey with leading dimension equal to the\n",
        "        length of the desired trajectory.\n",
        "      initial: A batch of initial states for the trajectories.\n",
        "\n",
        "    Returns:\n",
        "      An Unroll containing `len(rng_keys)` actions and `len(rng_keys) + 1`\n",
        "      states for each member of the batch.\n",
        "    \"\"\"\n",
        "    def loop_body(\n",
        "        states: States,\n",
        "        rng_key: chex.PRNGKey,\n",
        "    ) -> Tuple[States, Unroll]:\n",
        "      \"\"\"Samples an action from the policy, step the environment dynamics.\n",
        "\n",
        "      Args:\n",
        "        states: The current state(s) of the environment.\n",
        "        rng_key: The PRNGKey to use for this step.\n",
        "\n",
        "      Returns:\n",
        "        A tuple of the next state(s) of the environment and an `Unroll` pair\n",
        "        containing `states` and the action(s) sampled from it.\n",
        "      \"\"\"\n",
        "      actions = policy(rng_key, states)\n",
        "      new_states = self.transition(states, actions)\n",
        "\n",
        "      # N.B. states returned as part of unroll are the ones passed in as an\n",
        "      # argument. The new state is only passed to the next iteration.\n",
        "      return new_states, Unroll(actions=actions, states=states)\n",
        "\n",
        "    # We will want to concatenate the final state with the unroll, and thus\n",
        "    # we will end up with one more state than action.\n",
        "    final, unroll = jax.lax.scan(loop_body, initial, rng_keys)\n",
        "    all_states = jnp.concatenate([unroll.states, final[jnp.newaxis]])\n",
        "    return unroll.replace(states=all_states)\n",
        "\n",
        "\n",
        "def make_four_rooms() -> GridWorld:\n",
        "  return GridWorld(transition_tensor=transition_tensor(FOUR_ROOMS_GRID))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wgz-_ZWLO8rq"
      },
      "outputs": [],
      "source": [
        "#@title Plot an example trajectory (uniform random policy) { run: \"auto\" }\n",
        "\n",
        "trajectory_seed = 0  # @param { 'type': 'slider' }\n",
        "trajectory_length = 10  # @param { 'type': 'slider', 'min': 0, 'max': 50, 'step': 10 }\n",
        "\n",
        "def plot_example_trajectory():\n",
        "  \"\"\"Plot an example trajectory from the hyperparameters specified above.\"\"\"\n",
        "  env = make_four_rooms()\n",
        "  keys = jax.random.split(\n",
        "      jax.random.PRNGKey(trajectory_seed),\n",
        "      trajectory_length,\n",
        "  )\n",
        "  initial = env.initialize(())\n",
        "  # N.B. This will re-jit for different values of trajectory_length. Everything\n",
        "  # else should be fast to recompute.\n",
        "\n",
        "  policy = UniformRandomPolicy(env.num_actions)\n",
        "  trajectory = env.unroll(policy, keys[1:], initial)\n",
        "  grid = 1 - FOUR_ROOMS_GRID.astype(np.float64)\n",
        "  colors = np.linspace(0.8, 0.2, trajectory.states.shape[0])\n",
        "  state_integers = jax.device_get(\n",
        "      jnp.matmul(\n",
        "          trajectory.states,\n",
        "          jnp.arange(trajectory.states.shape[-1], dtype=jnp.int16),\n",
        "      ),\n",
        "  )\n",
        "  coordinates = FOUR_ROOMS_STATES[state_integers]\n",
        "  for coord, color in enumerate(colors):\n",
        "    grid_index = tuple(coordinates[coord])\n",
        "    grid[grid_index] = color\n",
        "  plt.matshow(grid, cmap=plt.cm.gray)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.title('Example trajectory (uniform policy)')\n",
        "\n",
        "plot_example_trajectory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aJvFN8LCVcLd"
      },
      "outputs": [],
      "source": [
        "# @title Model definitions\n",
        "\n",
        "# Some protocol definitons for the interfaces.\n",
        "\n",
        "class ConditionalQ(Protocol):\n",
        "  \"\"\"Interface for a (conditional) Q function.\"\"\"\n",
        "  def __call__(self, state: States, z: LatentCodes) -> QValues:\n",
        "    \"\"\"Compute Q-values (action values) for an environment state and code.\n",
        "\n",
        "    Args:\n",
        "      state: A (batch of) environment state(s).\n",
        "      z: A (batch of) environment latent codes.\n",
        "    Returns:\n",
        "      A (batch of) Q-values, with leading dimensions `state.shape[:-1]`\n",
        "      and a final axis with size equal to the number of environment actions.\n",
        "    \"\"\"\n",
        "\n",
        "class Discriminator(Protocol):\n",
        "  \"\"\"Interface for a mapping from environment states to latent codes.\"\"\"\n",
        "  def __call__(self, states: States) -> LatentLogProbs:\n",
        "    \"\"\"Predict a (batch of) latent code(s) from a (batch of) environment states.\n",
        "\n",
        "    Args:\n",
        "      states: A (batch of) environment state(s).\n",
        "    Returns:\n",
        "      A (batch of) log probabilities predicting latent code(s) corresponding to\n",
        "      environment state(s).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def maybe_tile_across_time(z: chex.Array, state: chex.Array) -> chex.Array:\n",
        "  \"\"\"Optionally `z` across first dimension if `state` is 3-dimensional.\"\"\"\n",
        "  if state.ndim == 3:\n",
        "    return jnp.tile(jnp.expand_dims(z, 0), [state.shape[0]] + [1] * z.ndim)\n",
        "  else:\n",
        "    return z\n",
        "\n",
        "\n",
        "class TabularConditionalQ(hk.Module):\n",
        "  \"\"\"A conditional Q function implemented as a lookup table.\"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      code_arity: int,\n",
        "      num_actions: int,\n",
        "      initializer: hk.initializers.Initializer = hk.initializers.RandomUniform(),\n",
        "      name: str = 'tabular_q',\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self._code_arity = code_arity\n",
        "    self._num_actions = num_actions\n",
        "    self._initializer = initializer\n",
        "\n",
        "  def __call__(self, state: States, z: LatentCodes) -> QValues:\n",
        "    shape = (self._code_arity, state.shape[-1], self._num_actions)\n",
        "    table = hk.get_parameter('table', shape, init=self._initializer)\n",
        "    z = maybe_tile_across_time(z, state)\n",
        "    chex.assert_equal_shape_prefix([z, state], z.ndim)\n",
        "    one_hot_z = jax.nn.one_hot(z, self._code_arity)\n",
        "    return jnp.einsum('...c,...s,csa->...a', one_hot_z, state, table)\n",
        "\n",
        "\n",
        "class TabularDiscriminatorEnsemble(hk.Module):\n",
        "  \"\"\"A discriminator ensemble implemented as a stack of lookup tables.\"\"\"\n",
        "  def __init__(\n",
        "      self,\n",
        "      ensemble_size: int,\n",
        "      code_arity: int,\n",
        "      initializer: hk.initializers.Initializer = hk.initializers.RandomUniform(),\n",
        "      name='discriminator',\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self._ensemble_size = ensemble_size\n",
        "    self._code_arity = code_arity\n",
        "    self._initializer = initializer\n",
        "\n",
        "  def __call__(self, states: States) -> LatentLogProbs:\n",
        "    shape = (self._ensemble_size, states.shape[-1], self._code_arity)\n",
        "    # Mimic Haiku's default initialization for linear layers.\n",
        "    if self._initializer is None:\n",
        "      stddev = np.sqrt(1 / states.shape[-1])\n",
        "      initializer = hk.initializers.TruncatedNormal(stddev=stddev)\n",
        "    else:\n",
        "      initializer = self._initializer\n",
        "    table = hk.get_parameter('table', shape, init=initializer)\n",
        "    # Fix overparameterization.\n",
        "    table *= (jnp.arange(states.shape[-1]) > 0).reshape((1, -1, 1))\n",
        "    logits = jnp.einsum('esc,...s->e...c', table, states)\n",
        "    return jax.nn.log_softmax(logits)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class Models:\n",
        "  \"\"\"A bundle of Q-functions and a discriminator.\"\"\"\n",
        "  skill_q: hk.Transformed\n",
        "  bonus_q: hk.Transformed\n",
        "  discriminator: hk.Transformed\n",
        "\n",
        "  @classmethod\n",
        "  def build(\n",
        "      cls,\n",
        "      env: GridWorld,\n",
        "      code_arity: int,\n",
        "      ensemble_size: int,\n",
        "  ) -> 'Models':\n",
        "    \"\"\"Build models from an environment and hyperparameters.\"\"\"\n",
        "\n",
        "    make_tabular_q = functools.partial(\n",
        "        TabularConditionalQ,\n",
        "        num_actions=env.num_actions,\n",
        "        code_arity=code_arity,\n",
        "    )\n",
        "    make_discriminators = functools.partial(\n",
        "        TabularDiscriminatorEnsemble,\n",
        "        ensemble_size,\n",
        "        code_arity,\n",
        "    )\n",
        "\n",
        "    # pylint: disable=unnecessary-lambda\n",
        "    return cls(\n",
        "        skill_q=hk.without_apply_rng(\n",
        "            hk.transform(lambda s, z: make_tabular_q(name='skill_q')(s, z)),\n",
        "        ),\n",
        "        bonus_q=hk.without_apply_rng(\n",
        "            hk.transform(lambda s, z: make_tabular_q(name='bonus_q')(s, z)),\n",
        "        ),\n",
        "        discriminator=hk.without_apply_rng(\n",
        "            hk.transform(\n",
        "                lambda s: make_discriminators()(s),\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    # pylint: enable=unnecessary-lambda\n",
        "\n",
        "\n",
        "  def init(\n",
        "      self,\n",
        "      env: GridWorld,\n",
        "      rng_key: chex.PRNGKey,\n",
        "  ) -> hk.Params:\n",
        "    \"\"\"Initialize both models, return a joint parameters container.\n",
        "\n",
        "    Args:\n",
        "      env: A `GridWorld`.\n",
        "      rng_key: A PRNGKey.\n",
        "    Returns:\n",
        "      A `hk.Params` containing the parameters for both the Q-function\n",
        "      and the discriminator, which can be passed to either `apply` function.\n",
        "    \"\"\"\n",
        "    discriminator_key, skill_key, bonus_key = jax.random.split(rng_key, 3)\n",
        "\n",
        "    # We know that the states will always be the same dimension.\n",
        "    states = jnp.zeros(env.state_dim, dtype=jnp.int32)\n",
        "\n",
        "    # Initialize the discriminator ensemble first, so we can encode our dummy state\n",
        "    # and states.\n",
        "    discriminator_params = self.discriminator.init(discriminator_key, states)\n",
        "    skill_q_params = self.skill_q.init(skill_key, states, jnp.array(0))\n",
        "    bonus_q_params = self.bonus_q.init(bonus_key, states, jnp.array(0))\n",
        "\n",
        "    # Merge together the parameter containers. The namespaces do not overlap\n",
        "    # so this will work with either transformed functions.\n",
        "    return hk.data_structures.merge(\n",
        "        discriminator_params,\n",
        "        skill_q_params,\n",
        "        bonus_q_params,\n",
        "    )\n",
        "\n",
        "  def with_params(\n",
        "      self,\n",
        "      params: hk.Params\n",
        "  ) -> Tuple[ConditionalQ, ConditionalQ, Discriminator]:\n",
        "    \"\"\"Return callables that curry (cache) a set of parameters for convenience.\n",
        "\n",
        "    Args:\n",
        "      params: A Haiku parameter set.\n",
        "    Returns:\n",
        "      A pair of `functools.partial` objects for the Q-network and predictor,\n",
        "      respectively, that respect the `ConditionalQNetwork` and `CodePredictor`\n",
        "      interfaces defined above.\n",
        "    \"\"\"\n",
        "    return tuple(functools.partial(getattr(self, n).apply, params)\n",
        "                 for n in ('skill_q', 'bonus_q', 'discriminator'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-FyXDpDEU3SA"
      },
      "outputs": [],
      "source": [
        "# @title Actor Loop\n",
        "\n",
        "@dataclasses.dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "  \"\"\"All of the constant configuration that doesn't change during training.\n",
        "\n",
        "  Bundle this together so that our top-level functions don't have a dozen\n",
        "  arguments and we don't rely on a cluttered global namespace, which leads to\n",
        "  confusing bugs.\n",
        "  \"\"\"\n",
        "  optimizer: optax.GradientTransformation\n",
        "  env: GridWorld\n",
        "  code_arity: int\n",
        "  ensemble_size: int\n",
        "  gamma: float\n",
        "  lambda_: float\n",
        "  bonus_weight: float\n",
        "  goal_duration: int\n",
        "  train_batch_size: int\n",
        "  train_epsilon: float\n",
        "  evaluation_epsilon: float\n",
        "  evaluation_batch_size: int\n",
        "\n",
        "  @property\n",
        "  @functools.lru_cache()\n",
        "  def models(self) -> Models:\n",
        "    return Models.build(\n",
        "        self.env,\n",
        "        self.code_arity,\n",
        "        self.ensemble_size,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingState:\n",
        "  \"\"\"All of the state that _does_ change during training.\"\"\"\n",
        "  params: hk.Params\n",
        "  opt_state: optax.OptState\n",
        "  rng_key: chex.PRNGKey\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Batch:\n",
        "  \"\"\"A batch of data generated by sampling code deltas and acting.\"\"\"\n",
        "  actions: Actions\n",
        "  states: States\n",
        "  codes: LatentCodes\n",
        "\n",
        "\n",
        "@functools.partial(jax.named_call, name='get_batch')\n",
        "def get_batch(\n",
        "    env: GridWorld,\n",
        "    models: Models,\n",
        "    state: TrainingState,\n",
        "    goal_duration: int,\n",
        "    bonus_weight: float,\n",
        "    epsilon: float,\n",
        "    codes: chex.Array,\n",
        ") -> Batch:\n",
        "  \"\"\"Act in the provided environment using the given network.\n",
        "\n",
        "  Args:\n",
        "    env: The `Environment` instance defining the dynamics.\n",
        "    models: A `Models` bundle of Q-functions and a discriminator.\n",
        "    state: The current `TrainingState`.\n",
        "    goal_duration: The number of actions to take in the environment\n",
        "      per goal period.\n",
        "    bonus_weight: Weight to assign to bonus Q function when selecting\n",
        "      actions greedily.\n",
        "    epsilon: Value to use for epsilon-greedy exploration.\n",
        "    codes: A 1-dimensional array of integer codes to condition (one per\n",
        "      batch item), values lying in the range [0, code_arity).\n",
        "\n",
        "  Returns:\n",
        "    A `Batch` of unrolled states, actions and the code deltas added\n",
        "    to the encoded initial state and used to condition the Q network when\n",
        "    acting.\n",
        "  \"\"\"\n",
        "  skill_q, bonus_q, _ = models.with_params(state.params)\n",
        "\n",
        "  # Split one key per step, plus one for sampling deltas.\n",
        "  step_keys = jax.random.split(state.rng_key, goal_duration)\n",
        "\n",
        "  batch_size = codes.shape[0]\n",
        "  env_state = env.initialize(batch_size)\n",
        "\n",
        "  @functools.partial(jax.named_call, name='batched_epsilon_greedy')\n",
        "  def batched_epsilon_greedy(key: chex.PRNGKey, states: jnp.ndarray) -> Actions:\n",
        "    \"\"\"Execute an epsilon greedy behavior policy conditioned on `desired_z`.\n",
        "\n",
        "    RLax epsilon greedy is unbatched, meaning we have to split the RNG key and\n",
        "    use `jax.vmap`.\n",
        "    \"\"\"\n",
        "    chex.assert_rank(states, 2)\n",
        "    batch_keys = jax.random.split(key, batch_size)\n",
        "    q_values = skill_q(states, codes) + bonus_weight * bonus_q(states, codes)\n",
        "    return jax.vmap(rlax.epsilon_greedy(epsilon).sample)(batch_keys, q_values)\n",
        "\n",
        "  unroll = env.unroll(batched_epsilon_greedy, step_keys, env_state)\n",
        "\n",
        "  # Chex dataclasses implementing the mapping interface by default, so we can\n",
        "  # use them just like dicts with the ** operator.\n",
        "  return Batch(codes=codes, **unroll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gGuH-J_KGg66"
      },
      "outputs": [],
      "source": [
        "#@title Loss\n",
        "\n",
        "PMAP_AXIS = 'devices'\n",
        "LOG2 = np.log(2)\n",
        "\n",
        "\n",
        "@chex.dataclass(frozen=True)\n",
        "class Statistics:\n",
        "  \"\"\"Statistics computed along with the loss.\"\"\"\n",
        "  skill_loss: float\n",
        "  bonus_loss: float\n",
        "  discriminator_loss: float\n",
        "  effective_skills: float\n",
        "  average_clipped_skill_reward: float\n",
        "  average_unweighted_bonus: float\n",
        "\n",
        "\n",
        "T = TypeVar('T', bound=chex.Array)\n",
        "\n",
        "\n",
        "def logmeanexp(x: T, axis: int) -> T:\n",
        "  \"\"\"Compute the log average of exponentiated quantities.\"\"\"\n",
        "  divisor = x.shape[axis]\n",
        "  return jnp.subtract(\n",
        "      jax.nn.logsumexp(x, axis=axis),\n",
        "      jnp.log(divisor),\n",
        "  )\n",
        "\n",
        "\n",
        "@functools.partial(jax.named_call, name='disdain_bonus')\n",
        "def disdain_bonus(ensemble_log_prob: LatentLogProbs) -> chex.Array:\n",
        "  \"\"\"Computes entropy of ensemble mean minus mean entropy of constituents.\"\"\"\n",
        "  chex.assert_rank(ensemble_log_prob, 3)  # (E, B, C)\n",
        "  average_discriminator_log_prob = logmeanexp(ensemble_log_prob, axis=0)\n",
        "  chex.assert_rank(average_discriminator_log_prob, 2)\n",
        "\n",
        "  @functools.partial(jax.named_call, name='entropy')\n",
        "  def entropy(log_prob: chex.Array) -> chex.Array:\n",
        "    return -(jnp.exp(log_prob) * log_prob).sum(axis=-1)\n",
        "\n",
        "  entropy_of_average_discriminator = entropy(average_discriminator_log_prob)\n",
        "  average_of_discriminator_entropies = entropy(ensemble_log_prob).mean(axis=0)\n",
        "  chex.assert_rank(entropy_of_average_discriminator, 1)\n",
        "  chex.assert_equal_shape([\n",
        "      entropy_of_average_discriminator,\n",
        "      average_of_discriminator_entropies,\n",
        "  ])\n",
        "  return entropy_of_average_discriminator - average_of_discriminator_entropies\n",
        "\n",
        "\n",
        "def sarsa_lambda(\n",
        "    q_tm1: chex.Array,\n",
        "    a_tm1: chex.Array,\n",
        "    r_t: chex.Array,\n",
        "    discount_t: chex.Array,\n",
        "    q_t: chex.Array,\n",
        "    a_t: chex.Array,\n",
        "    lambda_: chex.Numeric,\n",
        ") -> chex.Array:\n",
        "  \"\"\"SARSA(lambda) implementation from RLax slightly modified for speed.\n",
        "\n",
        "  The update performed on each Q function is equivalent to SARSA(lambda)\n",
        "  with `a_t` provided by the argmax action under the weighted combination\n",
        "  of Q functions.\n",
        "  \"\"\"\n",
        "  chex.assert_rank([q_tm1, a_tm1, r_t, discount_t, q_t, a_t, lambda_],\n",
        "                   [2, 1, 1, 1, 2, 1, {0, 1}])\n",
        "  chex.assert_type([q_tm1, a_tm1, r_t, discount_t, q_t, a_t, lambda_],\n",
        "                   [float, int, float, float, float, int, float])\n",
        "\n",
        "  # Much faster than the version in RLax for this size of problem on TPU.\n",
        "  def batched_index(q, a):\n",
        "    return (q * jax.nn.one_hot(a, q.shape[-1])).sum(axis=-1)\n",
        "\n",
        "  qa_tm1 = batched_index(q_tm1, a_tm1)\n",
        "  qa_t = batched_index(q_t, a_t)\n",
        "  target_tm1 = rlax.lambda_returns(r_t, discount_t, qa_t, lambda_)\n",
        "  return jax.lax.stop_gradient(target_tm1) - qa_tm1\n",
        "\n",
        "\n",
        "@functools.partial(jax.named_call, name='loss')\n",
        "def loss(\n",
        "    models: Models,\n",
        "    params: hk.Params,\n",
        "    batch: Batch,\n",
        "    gamma: float,\n",
        "    lambda_: float,\n",
        "    bonus_weight: float,\n",
        ") -> Tuple[float, Statistics]:\n",
        "  \"\"\"Compute the combined loss and summary statistics.\"\"\"\n",
        "  chex.assert_rank([\n",
        "      batch.codes,  # 1 scalar per batch\n",
        "      batch.actions,  # T scalars per batch\n",
        "      batch.states,  # T + 1 feature vectors per batch\n",
        "  ], [1, 2, 3])\n",
        "  chex.assert_equal_shape_suffix([batch.codes, batch.actions], 1)\n",
        "  chex.assert_equal_shape_prefix([batch.states[:-1], batch.actions], 2)\n",
        "\n",
        "  skill_q, bonus_q, discriminator = models.with_params(params)\n",
        "\n",
        "  ensemble_log_prob = discriminator(batch.states[-1])\n",
        "  chex.assert_rank(ensemble_log_prob, 3)\n",
        "  chex.assert_axis_dimension(ensemble_log_prob, 1, batch.actions.shape[-1])\n",
        "  average_log_prob = logmeanexp(ensemble_log_prob, axis=0)\n",
        "  code_arity = ensemble_log_prob.shape[-1]\n",
        "  one_hot_codes = jax.nn.one_hot(batch.codes, code_arity)\n",
        "  chex.assert_equal_shape([average_log_prob, one_hot_codes])\n",
        "  ensemble_log_likelihood = (one_hot_codes * average_log_prob).sum(axis=-1)\n",
        "  chex.assert_rank(ensemble_log_likelihood, 1)\n",
        "  skill_rewards = ensemble_log_likelihood + jnp.log(one_hot_codes.shape[-1])\n",
        "\n",
        "  bonus_rewards = disdain_bonus(ensemble_log_prob)\n",
        "  chex.assert_shape([skill_rewards, bonus_rewards], (batch.actions.shape[1],))\n",
        "\n",
        "  num_transitions = batch.actions.shape[0]\n",
        "  reward_mask = jnp.arange(num_transitions) == (num_transitions - 1)\n",
        "\n",
        "  # Do not bootstrap from the final state (discount 0).\n",
        "  # Discounts are the same for every batch item, so this has no batch dimension.\n",
        "  discounts = (1 - reward_mask) * gamma\n",
        "  skill_q_values = skill_q(batch.states, batch.codes)\n",
        "  bonus_q_values = bonus_q(batch.states, batch.codes)\n",
        "  combined = skill_q_values + bonus_weight * bonus_q_values\n",
        "  bootstrap_action = combined.argmax(axis=-1)\n",
        "\n",
        "  @functools.partial(jax.named_call, name='q_function_loss')\n",
        "  def q_function_loss(\n",
        "      q_values: chex.Array,\n",
        "      rewards: chex.Array,\n",
        "  ) -> chex.Scalar:\n",
        "    td_errors = jax.vmap(sarsa_lambda, [1, 1, 1, None, 1, 1, None])(\n",
        "        q_values[:-1],\n",
        "        batch.actions,\n",
        "        jnp.outer(reward_mask, rewards),\n",
        "        discounts,\n",
        "        q_values[1:],\n",
        "        bootstrap_action[1:],\n",
        "        lambda_,\n",
        "    )\n",
        "    return jnp.square(td_errors).sum(axis=-1).mean()\n",
        "\n",
        "  skill_loss = q_function_loss(skill_q_values, jnp.maximum(0, skill_rewards))\n",
        "  bonus_loss = q_function_loss(bonus_q_values, bonus_rewards)\n",
        "  discriminator_loss = -(one_hot_codes * ensemble_log_prob).sum(axis=-1).mean()\n",
        "\n",
        "  statistics = Statistics(\n",
        "      skill_loss=skill_loss,\n",
        "      bonus_loss=bonus_loss,\n",
        "      discriminator_loss=discriminator_loss,\n",
        "      effective_skills=jnp.power(2, skill_rewards.mean() / LOG2),\n",
        "      average_clipped_skill_reward=jnp.maximum(0, skill_rewards).mean(),\n",
        "      average_unweighted_bonus=bonus_rewards.mean(),\n",
        "  )\n",
        "  # Average the loss across devices.\n",
        "  statistics = jax.lax.pmean(statistics, PMAP_AXIS)\n",
        "  total_loss = jax.lax.pmean(\n",
        "      skill_loss + bonus_loss + discriminator_loss,\n",
        "      PMAP_AXIS,\n",
        "  )\n",
        "  total_loss = (\n",
        "      statistics.skill_loss +\n",
        "      statistics.bonus_loss +\n",
        "      statistics.discriminator_loss\n",
        "  )\n",
        "  return total_loss, statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dAlCEtG1P6vE"
      },
      "outputs": [],
      "source": [
        "# @title Training Step\n",
        "\n",
        "def _sample_codes(\n",
        "    rng_key: chex.PRNGKey,\n",
        "    batch_size: int,\n",
        "    code_arity: int,\n",
        ") -> LatentCodes:\n",
        "  \"\"\"Sample a batch of integer latent codes uniformly.\"\"\"\n",
        "  return jax.random.randint(\n",
        "      rng_key,\n",
        "      (batch_size,),\n",
        "      minval=0,\n",
        "      maxval=code_arity,\n",
        "  )\n",
        "\n",
        "\n",
        "def _step_on_batch(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        "    batch: Batch,\n",
        ") -> TrainingState:\n",
        "  \"\"\"Perform a single step of gradient descent on a data batch.\"\"\"\n",
        "  grad_loss = jax.grad(loss, argnums=1, has_aux=True)\n",
        "  device_grads, _ = grad_loss(\n",
        "      config.models,\n",
        "      state.params,\n",
        "      batch,\n",
        "      gamma=config.gamma,\n",
        "      lambda_=config.lambda_,\n",
        "      bonus_weight=config.bonus_weight,\n",
        "  )\n",
        "\n",
        "  # The gradient of an average should be the average of the gradients.\n",
        "  grads = jax.lax.pmean(device_grads, PMAP_AXIS)\n",
        "  updates, new_opt_state = config.optimizer.update(\n",
        "      grads,\n",
        "      state.opt_state,\n",
        "      state.params,\n",
        "  )\n",
        "\n",
        "  return state.replace(\n",
        "      params=optax.apply_updates(state.params, updates),\n",
        "      opt_state=new_opt_state,\n",
        "  )\n",
        "\n",
        "\n",
        "def _training_step(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        "    keys = Union[Tuple[chex.PRNGKey, chex.PRNGKey],\n",
        "                 Tuple[None, None]]\n",
        ") -> TrainingState:\n",
        "  \"\"\"Do a full training iteration: sample a data batch and update parameters.\"\"\"\n",
        "  act_key, codes_key = keys\n",
        "  if act_key is None or codes_key is None:\n",
        "    assert act_key == codes_key  # Both should be None or neither.\n",
        "    act_key, codes_key, next_key = jax.random.split(state.rng_key, 3)\n",
        "  else:\n",
        "    next_key = None\n",
        "\n",
        "  codes = _sample_codes(codes_key, config.train_batch_size, config.code_arity)\n",
        "\n",
        "  batch = get_batch(\n",
        "      config.env,\n",
        "      config.models,\n",
        "      state.replace(rng_key=act_key),\n",
        "      config.goal_duration,\n",
        "      config.bonus_weight,\n",
        "      config.train_epsilon,\n",
        "      codes,\n",
        "  )\n",
        "  return _step_on_batch(config, state, batch).replace(\n",
        "      rng_key=next_key if next_key is not None else state.rng_key,\n",
        "  )\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    axis_name=PMAP_AXIS,\n",
        "    static_broadcasted_argnums=0,\n",
        ")\n",
        "def training_step(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        ") -> TrainingState:\n",
        "  \"\"\"Simple `pmap`ed wrapper of `_training_step`.\"\"\"\n",
        "  return _training_step(config, state)\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    axis_name=PMAP_AXIS,\n",
        "    static_broadcasted_argnums=(0, 2),\n",
        ")\n",
        "def training_multistep(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        "    num_steps: int,\n",
        ") -> TrainingState:\n",
        "  \"\"\"Batch multiple training steps together for higher throughput.\"\"\"\n",
        "  keys = jax.random.split(state.rng_key, 2 * num_steps + 1)\n",
        "  new_state, _ = jax.lax.scan(\n",
        "      lambda s, key: (_training_step(config, s, key), None),\n",
        "      state,\n",
        "      (keys[:num_steps], keys[num_steps: 2 * num_steps]),\n",
        "  )\n",
        "  return new_state.replace(rng_key=keys[-1])\n",
        "\n",
        "\n",
        "@functools.partial(\n",
        "    jax.pmap,\n",
        "    axis_name=PMAP_AXIS,\n",
        "    static_broadcasted_argnums=0,\n",
        ")\n",
        "def evaluate(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        ") -> Statistics:\n",
        "  \"\"\"Gather a large batch of trajectories and evaluate diagnostics.\"\"\"\n",
        "  # We split the current key but don't replace it in the TrainingState.\n",
        "  # This means the training results are independent of how often we evaluate.\n",
        "\n",
        "  act_key, codes_key = jax.random.split(state.rng_key)\n",
        "  codes = _sample_codes(\n",
        "      codes_key,\n",
        "      config.evaluation_batch_size,\n",
        "      config.code_arity,\n",
        "  )\n",
        "\n",
        "  batch = get_batch(\n",
        "      config.env,\n",
        "      config.models,\n",
        "      state,\n",
        "      config.goal_duration,\n",
        "      config.bonus_weight,\n",
        "      config.evaluation_epsilon,\n",
        "      codes,\n",
        "  )\n",
        "  _, statistics = loss(\n",
        "      config.models,\n",
        "      state.params,\n",
        "      batch,\n",
        "      config.gamma,\n",
        "      config.lambda_,\n",
        "      config.bonus_weight,\n",
        "  )\n",
        "  assert statistics is not None\n",
        "  return statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lZHMjQkxVFUR"
      },
      "outputs": [],
      "source": [
        "#@title Model & Environment Setup\n",
        "# VIC goal duration.\n",
        "train_goal_duration = 8  # @param {'type': 'slider', 'min': 2, 'max': 100}\n",
        "\n",
        "# RL hyperparameters.\n",
        "train_epsilon = 0.001  # @param {'type': 'slider', 'min': 0.001, 'max': 0.1, 'step': 0.001}\n",
        "train_gamma = 0.99  # @param {'type': 'slider', 'min': 0.95, 'max': 0.999, 'step': 0.001}\n",
        "train_lambda = 0.7  # @param {'type': 'slider', 'min': 0.1, 'max': 0.9, 'step': 0.1}\n",
        "train_bonus_weight = 20  # @param {'type': 'slider', 'min': 0.0, 'max': 100.0, 'step': 1}\n",
        "\n",
        "# Network hyperparameters.\n",
        "train_code_arity = 24  # @param {'type': 'slider', 'min': 16, 'max': 128, 'step': 8}\n",
        "train_ensemble_size = 2  # @param {'type': 'slider', 'min': 2, 'max': 10, 'step': 1}\n",
        "\n",
        "# Optimizer hyperparameters.\n",
        "train_batch_size_per_device = 16  # @param\n",
        "train_learning_rate = 4e-3  # @param\n",
        "\n",
        "# Evaluation hyperparameters.\n",
        "evaluation_epsilon = 0.001  # @param {'type': 'slider', 'min': 0.000, 'max': 0.1, 'step': 0.001}\n",
        "evaluation_batch_size_per_device = 128  # @param {'type': 'integer'}\n",
        "disable_ensemble_in_baseline = True  # @param {'type': 'boolean'}\n",
        "\n",
        "\n",
        "def _build_config() -> TrainingConfig:\n",
        "  \"\"\"Put this in a function so we have less stuff in global namespace.\"\"\"\n",
        "  env = make_four_rooms()\n",
        "  optimizer = optax.sgd(train_learning_rate)\n",
        "  return TrainingConfig(\n",
        "      code_arity=train_code_arity,\n",
        "      ensemble_size=train_ensemble_size,\n",
        "      optimizer=optimizer,\n",
        "      env=env,\n",
        "      gamma=train_gamma,\n",
        "      lambda_=train_lambda,\n",
        "      bonus_weight=train_bonus_weight,\n",
        "      goal_duration=train_goal_duration,\n",
        "      train_batch_size=train_batch_size_per_device,\n",
        "      train_epsilon=train_epsilon,\n",
        "      evaluation_epsilon=evaluation_epsilon,\n",
        "      evaluation_batch_size=evaluation_batch_size_per_device,\n",
        "  )\n",
        "\n",
        "\n",
        "train_config = _build_config()\n",
        "baseline_config = dataclasses.replace(\n",
        "    train_config,\n",
        "    bonus_weight=0,\n",
        "    ensemble_size=(\n",
        "        1 if disable_ensemble_in_baseline else train_config.ensemble_size\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PH9nN7e6J7oa"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Training State (re-running starts from scratch)\n",
        "\n",
        "train_seed = 1  # @param {'type': 'integer'}\n",
        "\n",
        "def construct_state(\n",
        "    config: TrainingConfig,\n",
        ") -> TrainingState:\n",
        "  \"\"\"Build a bundle of mutable training state from a config and the seed.\"\"\"\n",
        "  init_key, train_key = jax.random.split(jax.random.PRNGKey(train_seed))\n",
        "  train_rng_key = jax.random.split(\n",
        "      train_key,\n",
        "      jax.local_device_count(),\n",
        "  )\n",
        "  params = jax.pmap(config.models.init, static_broadcasted_argnums=0)(\n",
        "      config.env,\n",
        "      jax.device_put_replicated(init_key, jax.local_devices()),\n",
        "  )\n",
        "  return TrainingState(\n",
        "      params=params,\n",
        "      opt_state=jax.pmap(config.optimizer.init)(params),\n",
        "      rng_key=train_rng_key,\n",
        "  )\n",
        "\n",
        "\n",
        "stats = baseline_stats = None\n",
        "initial_state = construct_state(train_config)\n",
        "train_state = construct_state(train_config)\n",
        "baseline_state = construct_state(baseline_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0Dw5zkloulRn"
      },
      "outputs": [],
      "source": [
        "#@title Training Loop (re-running cell continues)\n",
        "\n",
        "train_iterations = 1000000  # @param {'type': 'integer'}\n",
        "steps_per_call = 100000  # @param {'type': 'integer'}\n",
        "\n",
        "\n",
        "def log_statistics(iteration: int, statistics: Statistics, spaces: int = 2):\n",
        "  \"\"\"Log iteration number and statistics to output.\"\"\"\n",
        "  current_stats = dict(jax.tree_map(\n",
        "      operator.itemgetter(0),\n",
        "      jax.device_get(jax.tree_map(lambda x: x.block_until_ready(), statistics)),\n",
        "  ))\n",
        "  print(\n",
        "      'Iteration ',\n",
        "      str(iteration).rjust(math.ceil(1 + math.log10(train_iterations))),\n",
        "      (' ' * spaces).join(\n",
        "          f'{k} = %10.7f' % v\n",
        "          for k, v in current_stats.items()\n",
        "      )\n",
        "  )\n",
        "  sys.stdout.flush()\n",
        "\n",
        "\n",
        "print(('Training with goal duration %d, batch size %d on %d devices '\n",
        "       '(total per-step batch size %d, %d transitions per batch)') %\n",
        "      (train_goal_duration, train_batch_size_per_device,\n",
        "       jax.local_device_count(),\n",
        "       train_batch_size_per_device * jax.local_device_count(),\n",
        "       (jax.local_device_count() * train_batch_size_per_device *\n",
        "        train_goal_duration)))\n",
        "print('Evaluating on a batch of size %d per device '\n",
        "      '(for a total of %d trajectories per evaluation)' %\n",
        "      (evaluation_batch_size_per_device,\n",
        "       evaluation_batch_size_per_device * jax.local_device_count()))\n",
        "\n",
        "\n",
        "def run_training(\n",
        "    config: TrainingConfig,\n",
        "    state: TrainingState,\n",
        "    previous_stats: Optional[List[Tuple[int, Statistics]]] = None,\n",
        ") -> List[Tuple[int, Statistics]]:\n",
        "  \"\"\"Perform or possibly continue a training run.\n",
        "\n",
        "  Args:\n",
        "    config: A `TrainingConfig` specifying hyperparameters.\n",
        "    state: A `TrainingState` containing parameters and optimizer state.\n",
        "    previous_stats: An optional list of previously returned tuples of\n",
        "      (iteration number, statistics) tuples. If provided, iterations will\n",
        "      start after the last logged iteration number and results will be\n",
        "      concatenated into this list.\n",
        "\n",
        "  Returns:\n",
        "    A list of tuples of integer iteration number and `Statistics`,\n",
        "    containing as a prefix the contents of `previous_stats` if provided.\n",
        "  \"\"\"\n",
        "  train_stats = evaluate(config, state)\n",
        "  if previous_stats:\n",
        "    # Continue from last index if we've already trained.\n",
        "    start_index = stats[-1][0]\n",
        "    all_train_stats = previous_stats\n",
        "  else:\n",
        "    start_index = 0\n",
        "    all_train_stats = [(0, train_stats)]\n",
        "  log_statistics(start_index, train_stats)\n",
        "  print('Beginning training.')\n",
        "  t_start = datetime.datetime.now()\n",
        "  for step in range(\n",
        "      start_index + steps_per_call,\n",
        "      start_index + train_iterations + steps_per_call,\n",
        "      steps_per_call\n",
        "  ):\n",
        "    state = training_multistep(config, state, steps_per_call)\n",
        "    train_stats = evaluate(config, state)\n",
        "    log_statistics(step, train_stats)\n",
        "    all_train_stats.append((step, train_stats))\n",
        "  t_end = datetime.datetime.now()\n",
        "  print(f'Training took {t_end - t_start}')\n",
        "  return all_train_stats, state\n",
        "\n",
        "\n",
        "stats, train_state = run_training(train_config, train_state, stats)\n",
        "\n",
        "print('Running baseline with bonus weight 0 and ensemble size '\n",
        "      f'{baseline_config.ensemble_size}...')\n",
        "baseline_stats, baseline_state = run_training(\n",
        "    baseline_config,\n",
        "    baseline_state,\n",
        "    baseline_stats,\n",
        ")\n",
        "\n",
        "\n",
        "def fetch_one_copy(\n",
        "    s: List[Tuple[int, Statistics]]\n",
        ") -> List[Tuple[int, Statistics]]:\n",
        "  return [\n",
        "      (i, jax.tree_map(lambda x: x[0] if x.ndim > 0 else x, v))\n",
        "      for i, v in jax.device_get(s)\n",
        "  ]\n",
        "\n",
        "\n",
        "stats, baseline_stats = [fetch_one_copy(s) for s in (stats, baseline_stats)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AqUx25HAGJhG"
      },
      "outputs": [],
      "source": [
        "# @title Visualize Results\n",
        "def flatten_stats(\n",
        "    nested: List[Tuple[int, Statistics]],\n",
        ") -> Dict[str, List[float]]:\n",
        "  return {k: [getattr(v, k) for _, v in nested]\n",
        "          for k in Statistics.__dataclass_fields__}\n",
        "\n",
        "\n",
        "def comparison_plot(\n",
        "    flat_stats: Dict[str, List[float]],\n",
        "    flat_baseline: Dict[str, List[float]],\n",
        "    total_steps: int,\n",
        "    no_baseline_plots: Tuple[str, ...] = ('bonus_loss',),\n",
        "    label: str = 'DISDAIN',\n",
        "    baseline_label: str = 'No DISDAIN',\n",
        "    title: str = 'DISDAIN vs. matched hyperparameter baseline',\n",
        "    log_y: Tuple[str, ...] = ('skill_loss', 'bonus_loss'),\n",
        "    subplot_grid: Tuple[int, int] = (2, 3),\n",
        "    figure_size=(12, 6),\n",
        "):\n",
        "  \"\"\"Generate a paned comparison plot of statistics.\"\"\"\n",
        "  x_values = np.arange(0, total_steps + steps_per_call, steps_per_call)\n",
        "  plt.figure(figsize=figure_size)\n",
        "  for plot_num, k in enumerate(flat_stats.keys()):\n",
        "    plt.subplot(*subplot_grid, plot_num + 1)\n",
        "    if k in log_y:\n",
        "      plt.yscale('log')\n",
        "    plt.plot(x_values, flat_stats[k], label=label)\n",
        "    if k not in no_baseline_plots:\n",
        "      plt.plot(x_values, flat_baseline[k], label=baseline_label)\n",
        "    plt.legend()\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel(k.replace('_', ' '))\n",
        "  plt.suptitle(title)\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n",
        "comparison_plot(\n",
        "    flatten_stats(stats),\n",
        "    flatten_stats(baseline_stats),\n",
        "    total_steps=stats[-1][0]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uUC_uMCUuMQ4"
      },
      "outputs": [],
      "source": [
        "# @title Visualize State Occupancies {'run': 'auto'}\n",
        "\n",
        "@functools.partial(jax.pmap, static_broadcasted_argnums=(0, 1, 2))\n",
        "def stratified_skill_batch(\n",
        "    config: TrainingConfig,\n",
        "    epsilon: float,\n",
        "    trajectories_per_skill: int,\n",
        "    state: TrainingState,\n",
        "    rng_key: chex.PRNGKey,\n",
        ") -> Batch:\n",
        "  \"\"\"Get a batch of trajectories for each skill with a given epsilon.\"\"\"\n",
        "  def _get_batch(\n",
        "      state: TrainingState,\n",
        "      codes: chex.Array,\n",
        "  ) -> Batch:\n",
        "    return get_batch(\n",
        "        config.env,\n",
        "        config.models,\n",
        "        state,\n",
        "        goal_duration=config.goal_duration,\n",
        "        bonus_weight=config.bonus_weight,\n",
        "        epsilon=epsilon,\n",
        "        codes=codes,\n",
        "    )\n",
        "\n",
        "  vmapped = jax.vmap(_get_batch, in_axes=[None, 0])\n",
        "  codes = (\n",
        "      jnp.ones((trajectories_per_skill, 1), dtype=jnp.int32) *\n",
        "      jnp.arange(config.code_arity)\n",
        "  )\n",
        "  state = state.replace(rng_key=rng_key)\n",
        "  return vmapped(state, codes)\n",
        "\n",
        "\n",
        "seed = 0  # @param {type: 'integer'}\n",
        "visualization_epsilon = 0.001  # @param { type: 'slider', min: 0, max: 0.5, step: 0.001}\n",
        "trajectories_per_skill_per_device = 64  # @param { type: 'slider', min: 1, max: 64}\n",
        "colorbar_max = 0.3  # @param {type: 'slider', min: 0.2, max: 1.0, step: 0.05}\n",
        "\n",
        "\n",
        "key = jax.random.split(jax.random.PRNGKey(seed), jax.local_device_count())\n",
        "\n",
        "\n",
        "train_batch = stratified_skill_batch(\n",
        "    train_config,\n",
        "    visualization_epsilon,\n",
        "    trajectories_per_skill_per_device,\n",
        "    train_state,\n",
        "    key,\n",
        ")\n",
        "\n",
        "baseline_batch = stratified_skill_batch(\n",
        "    baseline_config,\n",
        "    visualization_epsilon,\n",
        "    trajectories_per_skill_per_device,\n",
        "    baseline_state,\n",
        "    key,\n",
        ")\n",
        "\n",
        "initial_batch = stratified_skill_batch(\n",
        "    train_config,\n",
        "    visualization_epsilon,\n",
        "    trajectories_per_skill_per_device,\n",
        "    initial_state,\n",
        "    key,\n",
        ")\n",
        "\n",
        "def histogram_map(\n",
        "    states_visited: chex.Array,\n",
        "    grid_shape: chex.Shape = FOUR_ROOMS_GRID.shape,\n",
        "    coordinate_map: chex.ArrayNumpy = FOUR_ROOMS_STATES,\n",
        ") -> chex.Array:\n",
        "  \"\"\"Generate a map of state frequencies in the shape of the grid world.\"\"\"\n",
        "  num_states = coordinate_map.shape[0]\n",
        "  counts = np.bincount(\n",
        "      jax.device_get(states_visited).argmax(-1).flatten(),\n",
        "      minlength=num_states,\n",
        "  )\n",
        "  hist = counts / counts.sum()\n",
        "  grid = np.zeros(grid_shape, dtype=np.float64)\n",
        "  r, c = coordinate_map[np.arange(num_states)].T\n",
        "  grid[r, c] = hist\n",
        "  return grid\n",
        "\n",
        "\n",
        "top = 'Initialization', 'DISDAIN', 'No DISDAIN'\n",
        "left = 'All Steps', 'Terminal States'\n",
        "data = initial_batch, train_batch, baseline_batch\n",
        "index = slice(None), (slice(None), slice(None), -1)\n",
        "\n",
        "hist_maps = [[], []]\n",
        "for i in range(2):\n",
        "  for j in range(3):\n",
        "    hist_maps[i].append(histogram_map(jax.device_get(data[j].states[index[i]])))\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 8))\n",
        "\n",
        "for i in range(2):\n",
        "  for j in range(3):\n",
        "    if i == 0:\n",
        "      axes[i, j].set_title(top[j])\n",
        "    if j == 0:\n",
        "      axes[i, j].set_ylabel(left[i])\n",
        "    im = axes[i, j].imshow(\n",
        "        hist_maps[i][j],\n",
        "        cmap=plt.cm.gist_gray_r,\n",
        "        vmin=0,\n",
        "        vmax=0.3,\n",
        "    )\n",
        "    axes[i, j].set_xticks([])\n",
        "    axes[i, j].set_yticks([])\n",
        "fig.colorbar(im, ax=axes.ravel().tolist())\n",
        "plt.suptitle('Fraction of steps spent in each state');"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "disdain.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

